# -*- coding: utf-8 -*-
"""explain_Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sC9noZvtewx911OnBZID2LgeDzzTONig

## Carga de datos y transformaciones

First, we import the pandas library, which is commonly used for data manipulation.
"""

import pandas as pd

"""We load the 'banking-conversation-corpus' dataset from Hugging Face, specifically the training split, which contains conversational data related to banking."""

from datasets import load_dataset

# https://huggingface.co/datasets/talkmap/banking-conversation-corpus

banking_conversation_corpus = load_dataset("talkmap/banking-conversation-corpus", split="train")
banking_conversation_corpus

"""This step filters out conversations where the 'text' field is None or its length is 10 characters or less. This helps to clean the dataset by removing irrelevant or empty entries."""

banking_conversation_corpus = banking_conversation_corpus.filter(lambda x: x['text'] is not None and len(x['text']) > 10)
banking_conversation_corpus

"""To work with a smaller, manageable portion of the dataset, we create a sample containing the first 100,000 records. This is useful for faster experimentation and development."""

banking_conversation_corpus_sample = banking_conversation_corpus[:100000]

"""This block processes the sampled conversations using pandas to group them by `conversation_id`, sorts them chronologically, and concatenates individual speaker turns into a single `full_conversation` string. This prepares the data for embedding by creating a unified text representation for each conversation."""

import pandas as pd

# Convert the dictionary sample to a pandas DataFrame directly
df = pd.DataFrame(banking_conversation_corpus_sample)

import pandas as pd

# 2. Ordenar cronológicamente
# Es indispensable para que el diálogo sea coherente.
df = df.sort_values(by=['conversation_id', 'date_time'])

# 3. Formateo Vectorizado (Mucho más rápido que .apply)
# Creamos la línea "SPEAKER: mensaje" usando operaciones nativas de strings
df['temp_line'] = df['speaker'].str.upper() + ": " + df['text'].fillna('')

# 4. Agrupación y Concatenación
# Usamos el método 'join' directamente sobre el objeto agrupado
df_conversations = (
    df.groupby('conversation_id', sort=False)['temp_line']
    .apply(lambda x: "\n".join(x))
    .reset_index(name='full_conversation')
)

# 5. Limpieza de memoria
# Eliminamos la columna temporal del dataframe original si es necesario
df.drop(columns=['temp_line'], inplace=True)

# Visualización del resultado
print(df_conversations.head())

# Guardar resultado (Parquet es mejor para archivos grandes)
# df_conversations.to_parquet('conversations_merged.parquet', index=False)

"""After consolidating conversations into a pandas DataFrame, we convert it back into a Hugging Face Dataset object. This allows us to continue using the efficient dataset operations provided by the `datasets` library."""

from datasets import Dataset

banking_conversation_corpus = Dataset.from_pandas(df_conversations)
banking_conversation_corpus

"""## Creación de los embeddings

We load a pre-trained tokenizer and model from the `sentence-transformers/all-MiniLM-L6-v2` checkpoint. This model is designed to produce high-quality sentence embeddings, and the tokenizer prepares text inputs for this model.
"""

from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)

"""To accelerate computations, especially for large models and datasets, we move the model to the GPU (CUDA device) if available. This leverages the parallel processing capabilities of GPUs."""

import torch

device = torch.device("cuda")
model.to(device)

"""**This** function defines a common pooling strategy for transformer models. It extracts the embedding of the `[CLS]` token (the first token in the sequence), which often represents the overall sentence meaning, from the model's last hidden state."""

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]

"""The `get_embeddings` function takes a list of text inputs, tokenizes them, moves them to the specified device, passes them through the model, and then applies CLS pooling to obtain a single embedding vector for each text."""

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)

"""We apply the `get_embeddings` function to the entire `banking_conversation_corpus` dataset using the `map` function. This efficiently generates an embedding for each conversation and adds it as a new 'embeddings' column to the dataset."""

embedding = get_embeddings(banking_conversation_corpus["full_conversation"][0])
embedding.shape

embeddings_dataset = banking_conversation_corpus.map(
    lambda x: {"embeddings": get_embeddings(x["full_conversation"]).detach().cpu().numpy()[0]}
)

"""## Uso de FAISS para busquedas

This cell installs `faiss-cpu` and then adds a FAISS index to the `embeddings_dataset` based on the 'embeddings' column. FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors, which significantly speeds up nearest-neighbor queries.
"""

embeddings_dataset.add_faiss_index(column="embeddings")

"""Here, we define a sample `question` and generate its embedding using the `get_embeddings` function. This question embedding will be used to query our dataset for similar conversations."""

question = "calling Union Financial"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape

"""This cell performs a similarity search on the `embeddings_dataset` using the `question_embedding`. It retrieves the 5 nearest examples (conversations) based on their embedding similarity, along with their corresponding similarity scores."""

scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)

"""The retrieved samples and their scores are converted into a pandas DataFrame. The DataFrame is then sorted by `scores` in descending order to easily view the most similar conversations first."""

import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)

"""Finally, this loop iterates through the sorted DataFrame of similar conversations, printing each conversation's text and its similarity score. This allows us to inspect the results of our semantic search."""

for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.full_conversation}")
    print(f"SCORE: {row.scores}")
    print("=" * 50)
    print()